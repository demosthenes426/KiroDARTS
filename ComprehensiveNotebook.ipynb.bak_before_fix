{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Training, Evaluation, and Saving\n",
    "\n",
    "This notebook walks through an end-to-end pipeline:\n",
    "1. Loads real data from `Data/covariaterawdata1.csv`.\n",
    "2. Processes, splits, and scales the data.\n",
    "3. Creates and trains multiple Darts models.\n",
    "4. **Prints per-epoch training and validation loss.**\n",
    "5. **Evaluates each model's predictions** against the test set and reports metrics (MAPE, RMSE, R²).\n",
    "6. **Saves the trained model and its scaler** to the `model_artifacts` directory.\n",
    "7. Plots a sample prediction for the best-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd",
    "import numpy as np",
    "import sys",
    "import os",
    "import warnings",
    "",
    "# Suppress warnings for a cleaner notebook",
    "warnings.filterwarnings('ignore', category=UserWarning)",
    "",
    "# Add src directory to the Python path to import custom modules",
    "src_path = os.path.join(os.getcwd(), 'src')",
    "if src_path not in sys.path:",
    "    sys.path.insert(0, src_path)",
    "",
    "# Import all necessary components from the src directory",
    "from darts_timeseries_creator import DartsTimeSeriesCreator",
    "from model_factory import ModelFactory",
    "from model_trainer import ModelTrainer",
    "from data_splitter import DataSplitter",
    "from data_scaler import DataScaler",
    "from results_visualizer import ResultsVisualizer",
    "from model_artifact_saver import ModelArtifactSaver",
    "from model_evaluator import ModelEvaluator",
    "",
    "print(\"Setup complete. All modules imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_file = os.path.join(os.getcwd(), 'Data', 'covariaterawdata1.csv')",
    "",
    "if os.path.exists(real_data_file):",
    "    df = pd.read_csv(real_data_file)",
    "    df['date'] = pd.to_datetime(df['date'])",
    "    df = df.set_index('date').sort_index()",
    "    ",
    "    numeric_df = df.select_dtypes(include=[np.number])",
    "    clean_df = numeric_df.dropna()",
    "    ",
    "    # Using a subset for a quick test. Remove .head() to use the full dataset.",
    "    data_df = clean_df.head(300)",
    "    ",
    "    print(f\"Loaded real data: {len(data_df)} rows, {len(data_df.columns)} columns\")",
    "    print(f\"Date range: {data_df.index.min()} to {data_df.index.max()}\")",
    "    display(data_df.head())",
    "else:",
    "    print(f\"Error: Real data file not found at {real_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Darts TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_creator = DartsTimeSeriesCreator()",
    "timeseries = timeseries_creator.create_timeseries(data_df)",
    "ts_info = timeseries_creator.get_timeseries_info(timeseries)",
    "print(f\"Successfully created TimeSeries with {ts_info['length']} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split and Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splitter = DataSplitter(val_size=0.15, test_size=0.15)",
    "train_ts, val_ts, test_ts = data_splitter.split_data(timeseries)",
    "print(f\"Data split: Train={len(train_ts)}, Val={len(val_ts)}, Test={len(test_ts)}\")",
    "",
    "data_scaler = DataScaler()",
    "train_scaled, val_scaled, test_scaled, scaler = data_scaler.scale_data(train_ts, val_ts, test_ts)",
    "print(\"Data scaled successfully. The scaler object is ready to be saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create, Train, and Evaluate Models\n",
    "\n",
    "This is the main loop. For each model specified, it will:\n",
    "1. Train the model (outputting per-epoch loss).\n",
    "2. Generate predictions on the test set.\n",
    "3. Evaluate the predictions and store the metrics.\n",
    "4. Save the trained model and the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---",
    "INPUT_CHUNK_LENGTH = 20",
    "OUTPUT_CHUNK_LENGTH = 5 # Predict 5 days ahead",
    "N_EPOCHS = 15 # Increase for better accuracy",
    "",
    "# --- Component Initialization ---",
    "model_factory = ModelFactory(INPUT_CHUNK_LENGTH, OUTPUT_CHUNK_LENGTH, n_epochs=N_EPOCHS, random_state=42)",
    "model_trainer = ModelTrainer(max_epochs=N_EPOCHS, verbose=True) # verbose=True shows per-epoch loss",
    "artifact_saver = ModelArtifactSaver(artifact_dir='model_artifacts')",
    "model_evaluator = ModelEvaluator()",
    "",
    "# --- Model Selection ---",
    "# You can add more models here from the factory: ['RNNModel', 'DLinearModel', 'NBEATSModel', 'TFTModel', 'BlockRNNModel']",
    "models_to_test = ['RNNModel', 'DLinearModel', 'NBEATSModel']",
    "",
    "# --- Execution Loop ---",
    "all_created_models = model_factory.create_models()",
    "trained_models = {}",
    "evaluation_results = {}",
    "",
    "for model_name in models_to_test:",
    "    print(f'\n{'{'}\"=\"*50}")",
    "    print(f'--- Processing Model: {model_name} ---')",
    "    print(f'='*50 + '\n')",
    "    ",
    "    model = all_created_models.get(model_name)",
    "    if not model:",
    "        print(f'✗ Model {model_name} not found in factory. Skipping.')",
    "        continue",
    "",
    "    # 1. Train the model",
    "    try:",
    "        training_result = model_trainer.train_model(model, train_scaled, val_scaled, model_name=model_name)",
    "        trained_models[model_name] = model",
    "        print(f'✓ {model_name} training completed in {training_result.training_time:.2f}s')",
    "    except Exception as e:",
    "        print(f'✗ {model_name} training failed: {e}')",
    "        continue # Skip to the next model",
    "",
    "    # 2. Generate predictions",
    "    print(f'\n--- Evaluating {model_name} ---')",
    "    try:",
    "        prediction = model.predict(n=len(test_ts), series=train_scaled)",
    "        ",
    "        # 3. Evaluate predictions",
    "        # Rescale for evaluation",
    "        prediction_rescaled = scaler.inverse_transform(prediction)",
    "        test_ts_rescaled = scaler.inverse_transform(test_ts)",
    "        ",
    "        # We evaluate on the 'adj_close' target variable",
    "        target_col = 'adj_close'",
    "        eval_results = model_evaluator.evaluate(",
    "            actual_series=test_ts_rescaled[target_col],",
    "            predicted_series=prediction_rescaled[target_col]",
    "        )",
    "        evaluation_results[model_name] = eval_results",
    "        print(f'✓ Evaluation complete for {model_name}.')",
    "        for metric, value in eval_results.items():",
    "            print(f'  - {metric}: {value:.4f}')",
    "",
    "        # 4. Save artifacts",
    "        artifact_saver.save_model_and_scaler(model, scaler, model_name)",
    "        print(f'✓ Model and scaler for {model_name} saved successfully.')",
    "",
    "    except Exception as e:",
    "        print(f'✗ Prediction or Evaluation failed for {model_name}: {e}')",
    "",
    "print('\n--- All models processed ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Review Evaluation Results\n",
    "\n",
    "Let's display the evaluation metrics in a summary table to easily compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_results:",
    "    results_df = pd.DataFrame(evaluation_results).T # Transpose to have models as rows",
    "    print(\"--- Model Performance Summary ---")",
    "    display(results_df)",
    "    ",
    "    # Find the best model based on a chosen metric (e.g., MAPE - lower is better)",
    "    best_model_name = results_df['mape'].idxmin()",
    "    print(f'\nBest performing model (based on lowest MAPE): {best_model_name}')",
    "else:",
    "    print(\"No models were successfully evaluated.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualize Best Model's Prediction\n",
    "\n",
    "Finally, we'll take the best performing model and visualize its predictions against the actual values from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResultsVisualizer(output_dir='output/plots')",
    "",
    "if 'best_model_name' in locals() and best_model_name in trained_models:",
    "    best_model = trained_models[best_model_name]",
    "    ",
    "    print(f'--- Generating prediction plot for {best_model_name} ---')",
    "    ",
    "    # Generate prediction",
    "    prediction = best_model.predict(n=len(test_ts), series=train_scaled)",
    "    ",
    "    # Rescale for plotting",
    "    prediction_rescaled = scaler.inverse_transform(prediction)",
    "    test_ts_rescaled = scaler.inverse_transform(test_ts)",
    "    ",
    "    # Extract the 'adj_close' column for plotting",
    "    prediction_to_plot = prediction_rescaled['adj_close']",
    "    actual_to_plot = test_ts_rescaled['adj_close']",
    "    ",
    "    # Plot the results",
    "    visualizer.plot_predictions(",
    "        model_name=f'BEST_MODEL_{best_model_name}', ",
    "        target_series=actual_to_plot, ",
    "        prediction_series=prediction_to_plot",
    "    )",
    "    print(f'✓ Prediction plot for {best_model_name} saved to output/plots/')",
    "else:",
    "    print('Could not determine the best model, skipping visualization.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

